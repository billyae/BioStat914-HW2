{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD for a Noisy Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix Shape: (100, 50)\n"
     ]
    }
   ],
   "source": [
    "# Generate the matrix\n",
    "\n",
    "N,M = 100,50\n",
    "\n",
    "# Form 2 random vectors\n",
    "u=np.random.rand(N,1)\n",
    "v=np.random.rand(M,1)\n",
    "\n",
    "# Compute the rank-1 matrix using outer product\n",
    "R_f = u @ v.T\n",
    "\n",
    "R_f_variannce = np.var(R_f)\n",
    "\n",
    "noise_variance = 0.01 * R_f_variannce\n",
    "\n",
    "noise = np.random.normal(0, np.sqrt(noise_variance), (N,M)) \n",
    "\n",
    "R = R_f + noise\n",
    "\n",
    "# Display the shape of the matrix\n",
    "\n",
    "print(\"The matrix Shape:\", R.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U Shape: (100, 50)\n",
      "s Shape: (50,)\n",
      "V Shape: (50, 50)\n"
     ]
    }
   ],
   "source": [
    "# Decompose the matrix using SVD\n",
    "\n",
    "U, s, V = np.linalg.svd(R, full_matrices=False)\n",
    "\n",
    "print(\"U Shape:\", U.shape)\n",
    "\n",
    "print(\"s Shape:\", s.shape)\n",
    "\n",
    "print(\"V Shape:\", V.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the matrix R using only the first singular value and the corresponidng singular vectors\n",
    "\n",
    "R_reconstructed = s[0] * np.outer(U[:,0], V[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed U: [[-1.58853015e-01 -9.03305987e-01 -2.05647825e-02 ...  9.99250643e-04\n",
      "   1.29078554e-02  3.90735679e-01]\n",
      " [-3.61870429e-02  1.82662444e-02 -6.84124603e-03 ... -3.86679075e-02\n",
      "  -1.87485907e-01  2.28802546e-02]\n",
      " [-7.41074272e-02  4.82230489e-02  1.77721037e-01 ... -2.09417701e-01\n",
      "   1.44309495e-01  9.01980436e-02]\n",
      " ...\n",
      " [-9.43860307e-02 -2.30633290e-02 -7.43519931e-02 ...  1.08672757e-01\n",
      "   2.40216060e-01 -1.03382094e-01]\n",
      " [-1.04698953e-01  1.49053296e-02 -5.87016611e-02 ...  9.77849463e-02\n",
      "  -1.25084030e-01  5.44218714e-03]\n",
      " [-1.29455555e-01  4.25009123e-04  2.74446929e-02 ... -1.27965451e-01\n",
      "   8.77236311e-02 -7.99790072e-02]]\n",
      "Reconstructed V: [[-1.90080103e-01 -1.39387274e-01 -1.48969266e-01 ... -7.55533737e-02\n",
      "  -2.34619178e-01 -7.53913079e-02]\n",
      " [-4.55665366e-01 -1.46359149e-01  2.96844473e-02 ...  7.92121352e-02\n",
      "   6.44868590e-02  3.51972772e-02]\n",
      " [ 0.00000000e+00 -2.08852729e-01  6.23098318e-02 ...  4.43360141e-03\n",
      "  -1.77167039e-01  1.16023548e-01]\n",
      " ...\n",
      " [ 0.00000000e+00 -1.31683752e-01  2.01130239e-03 ... -3.81015400e-03\n",
      "   1.82883882e-02  4.80454492e-02]\n",
      " [ 0.00000000e+00 -7.57754200e-04  2.23985770e-02 ...  3.06128681e-02\n",
      "  -5.50219221e-03  3.52017438e-03]\n",
      " [ 8.69619819e-01 -1.07156646e-01 -1.70073388e-02 ...  2.49914193e-02\n",
      "  -1.74927122e-02  1.96383821e-03]]\n"
     ]
    }
   ],
   "source": [
    "# output the Reconstructed values of U and V\n",
    "\n",
    "U_reconstructed, s_constructed, V_reconstructed = np.linalg.svd(R_reconstructed, full_matrices=False)\n",
    "\n",
    "U_no_noise, s_no_noise, V_no_noise = np.linalg.svd(R_f, full_matrices=False)\n",
    "\n",
    "\n",
    "print(\"Reconstructed U:\", U_reconstructed)\n",
    "\n",
    "print(\"Reconstructed V:\", V_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.004113459642564886\n",
      "RMSE_U: 0.1400753762639409\n",
      "RMSE_V: 0.19980025146763342\n"
     ]
    }
   ],
   "source": [
    "# Computer the Root Mean Squared Error between the original matrix R and the reconstructed matrix R_reconstructed \n",
    "\n",
    "RMSE = np.sqrt(np.mean((R_f - R_reconstructed)**2))\n",
    "\n",
    "print(\"RMSE:\", RMSE)\n",
    "\n",
    "# Computer the Root Mean Squared Errors of U and V\n",
    "\n",
    "RMSE_U = np.sqrt(np.mean((U_no_noise - U_reconstructed)**2))\n",
    "\n",
    "RMSE_V = np.sqrt(np.mean((V_no_noise - V_reconstructed)**2))\n",
    "\n",
    "print(\"RMSE_U:\", RMSE_U)\n",
    "print(\"RMSE_V:\", RMSE_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The impact of noise:** : \n",
    "\n",
    "1. The small RMSE between Rf and reconstructed R suggests that overall matrix structure remains well-preserved.  \n",
    "2. The high RMSE values for u and v indicate that noise impacts the individual components significantly.  \n",
    "3. This shows that though the matrix structure can be largely recovered, the exact original vectors u and v is more difficult due to the added noises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization of an Imcomplete Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the Matrix \n",
    "\n",
    "# Calculate the total number of elements and 30% of that\n",
    "total_elements = N * M\n",
    "num_missing = int(0.3 * total_elements)\n",
    "\n",
    "# Randomly select indices to set as missing\n",
    "missing_indices = np.unravel_index(\n",
    "    np.random.choice(total_elements, num_missing, replace=False), (N, M)\n",
    ")\n",
    "\n",
    "# Create a copy of the original matrix and set the missing values to nan\n",
    "\n",
    "R_missing = np.copy(R_f)\n",
    "\n",
    "R_missing[missing_indices] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 249.76260322806968\n",
      "Epoch: 10 Loss: 22.78780964872795\n",
      "Epoch: 20 Loss: 1.7995300314279858\n",
      "Epoch: 30 Loss: 0.1710255187868417\n",
      "Epoch: 40 Loss: 0.018692102559455345\n",
      "Epoch: 50 Loss: 0.002294365948007942\n",
      "Epoch: 60 Loss: 0.0003098107080645085\n",
      "Epoch: 70 Loss: 4.501471812531755e-05\n",
      "Epoch: 80 Loss: 6.896587272745627e-06\n",
      "Epoch: 90 Loss: 1.0964224478645974e-06\n",
      "Epoch: 100 Loss: 1.7881771314641975e-07\n",
      "Epoch: 110 Loss: 2.968768157752536e-08\n",
      "Epoch: 120 Loss: 4.9919760603810695e-09\n",
      "Epoch: 130 Loss: 8.473464279519105e-10\n",
      "Epoch: 140 Loss: 1.4487488658712918e-10\n",
      "Epoch: 150 Loss: 2.4913198223509648e-11\n",
      "Epoch: 160 Loss: 4.30453818095305e-12\n",
      "Epoch: 170 Loss: 7.467341055041587e-13\n",
      "Epoch: 180 Loss: 1.2999028249484452e-13\n",
      "Epoch: 190 Loss: 2.2697534946680503e-14\n",
      "Epoch: 200 Loss: 3.973964593635715e-15\n",
      "Epoch: 210 Loss: 6.974734612706469e-16\n",
      "Epoch: 220 Loss: 1.2268455395745962e-16\n",
      "Epoch: 230 Loss: 2.162346511087704e-17\n",
      "Epoch: 240 Loss: 3.818205581980371e-18\n",
      "Epoch: 250 Loss: 6.753478689137359e-19\n",
      "Epoch: 260 Loss: 1.1963871491884604e-19\n",
      "Epoch: 270 Loss: 2.1225286784560585e-20\n",
      "Epoch: 280 Loss: 3.770735488146353e-21\n",
      "Epoch: 290 Loss: 6.70731982687699e-22\n",
      "Epoch: 300 Loss: 1.1945654722548127e-22\n",
      "Epoch: 310 Loss: 2.1325366193598686e-23\n",
      "Epoch: 320 Loss: 3.831323693941424e-24\n",
      "Epoch: 330 Loss: 7.046130410576367e-25\n",
      "Epoch: 340 Loss: 1.4473937022084755e-25\n",
      "Epoch: 350 Loss: 4.305744656771236e-26\n",
      "Epoch: 360 Loss: 2.2240954185743642e-26\n",
      "Epoch: 370 Loss: 1.6199483161329238e-26\n",
      "Epoch: 380 Loss: 1.3827623172323334e-26\n",
      "Epoch: 390 Loss: 1.320209957122985e-26\n",
      "Epoch: 400 Loss: 1.3151333443615562e-26\n",
      "Epoch: 410 Loss: 1.314228007157376e-26\n",
      "Epoch: 420 Loss: 1.314228007157376e-26\n",
      "Epoch: 430 Loss: 1.314228007157376e-26\n",
      "Epoch: 440 Loss: 1.314228007157376e-26\n",
      "Epoch: 450 Loss: 1.314228007157376e-26\n",
      "Epoch: 460 Loss: 1.314228007157376e-26\n",
      "Epoch: 470 Loss: 1.314228007157376e-26\n",
      "Epoch: 480 Loss: 1.314228007157376e-26\n",
      "Epoch: 490 Loss: 1.314228007157376e-26\n",
      "Epoch: 500 Loss: 1.314228007157376e-26\n",
      "Epoch: 510 Loss: 1.314228007157376e-26\n",
      "Epoch: 520 Loss: 1.314228007157376e-26\n",
      "Epoch: 530 Loss: 1.314228007157376e-26\n",
      "Epoch: 540 Loss: 1.314228007157376e-26\n",
      "Epoch: 550 Loss: 1.314228007157376e-26\n",
      "Epoch: 560 Loss: 1.314228007157376e-26\n",
      "Epoch: 570 Loss: 1.314228007157376e-26\n",
      "Epoch: 580 Loss: 1.314228007157376e-26\n",
      "Epoch: 590 Loss: 1.314228007157376e-26\n",
      "Epoch: 600 Loss: 1.314228007157376e-26\n",
      "Epoch: 610 Loss: 1.314228007157376e-26\n",
      "Epoch: 620 Loss: 1.314228007157376e-26\n",
      "Epoch: 630 Loss: 1.314228007157376e-26\n",
      "Epoch: 640 Loss: 1.314228007157376e-26\n",
      "Epoch: 650 Loss: 1.314228007157376e-26\n",
      "Epoch: 660 Loss: 1.314228007157376e-26\n",
      "Epoch: 670 Loss: 1.314228007157376e-26\n",
      "Epoch: 680 Loss: 1.314228007157376e-26\n",
      "Epoch: 690 Loss: 1.314228007157376e-26\n",
      "Epoch: 700 Loss: 1.314228007157376e-26\n",
      "Epoch: 710 Loss: 1.314228007157376e-26\n",
      "Epoch: 720 Loss: 1.314228007157376e-26\n",
      "Epoch: 730 Loss: 1.314228007157376e-26\n",
      "Epoch: 740 Loss: 1.314228007157376e-26\n",
      "Epoch: 750 Loss: 1.314228007157376e-26\n",
      "Epoch: 760 Loss: 1.314228007157376e-26\n",
      "Epoch: 770 Loss: 1.314228007157376e-26\n",
      "Epoch: 780 Loss: 1.314228007157376e-26\n",
      "Epoch: 790 Loss: 1.314228007157376e-26\n",
      "Epoch: 800 Loss: 1.314228007157376e-26\n",
      "Epoch: 810 Loss: 1.314228007157376e-26\n",
      "Epoch: 820 Loss: 1.314228007157376e-26\n",
      "Epoch: 830 Loss: 1.314228007157376e-26\n",
      "Epoch: 840 Loss: 1.314228007157376e-26\n",
      "Epoch: 850 Loss: 1.314228007157376e-26\n",
      "Epoch: 860 Loss: 1.314228007157376e-26\n",
      "Epoch: 870 Loss: 1.314228007157376e-26\n",
      "Epoch: 880 Loss: 1.314228007157376e-26\n",
      "Epoch: 890 Loss: 1.314228007157376e-26\n",
      "Epoch: 900 Loss: 1.314228007157376e-26\n",
      "Epoch: 910 Loss: 1.314228007157376e-26\n",
      "Epoch: 920 Loss: 1.314228007157376e-26\n",
      "Epoch: 930 Loss: 1.314228007157376e-26\n",
      "Epoch: 940 Loss: 1.314228007157376e-26\n",
      "Epoch: 950 Loss: 1.314228007157376e-26\n",
      "Epoch: 960 Loss: 1.314228007157376e-26\n",
      "Epoch: 970 Loss: 1.314228007157376e-26\n",
      "Epoch: 980 Loss: 1.314228007157376e-26\n",
      "Epoch: 990 Loss: 1.314228007157376e-26\n"
     ]
    }
   ],
   "source": [
    "# SGD for matrix factorization\n",
    "\n",
    "def SGD_factorization(learning_rate, regularization, num_epochs, R_missing, output, Rank):\n",
    "\n",
    "    K= Rank\n",
    "    U_factorized = np.random.rand(N,K)\n",
    "    V_factorized = np.random.rand(M,K) # initialze the U and V matrices\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for i in range(N):\n",
    "            for j in range(M):\n",
    "                \n",
    "                # Only consider the observed values\n",
    "                if not np.isnan(R_missing[i,j]):\n",
    "\n",
    "                    prediction = np.dot(U_factorized[i,:], V_factorized[j,:])\n",
    "                    error = R_missing[i,j] - prediction\n",
    "\n",
    "                    # Update the U and V matrices\n",
    "                    U_factorized[i,:] += learning_rate * (error * V_factorized[j,:] - regularization * U_factorized[i,:])\n",
    "                    V_factorized[j,:] += learning_rate * (error * U_factorized[i,:] - regularization * V_factorized[j,:]) \n",
    "                    \n",
    "        if output:\n",
    "            if epoch % 10 == 0:\n",
    "            \n",
    "                # Calculatre the total loss on observed entrices\n",
    "                observed_indices = ~np.isnan(R_missing)\n",
    "                loss = np.sum((R_missing[observed_indices] - (U_factorized @ V_factorized.T)[observed_indices])**2)\n",
    "                print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "    \n",
    "    return U_factorized, V_factorized\n",
    "\n",
    "U_factorized,V_factorized = SGD_factorization(0.01, 0, 1000, R_missing,True,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the factorized matrices U and V to reconstruct the matrix R\n",
    "\n",
    "R_reconstructed_missing = U_factorized @ V_factorized.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8864491 ]\n",
      " [0.1963928 ]\n",
      " [0.42000427]\n",
      " [0.45195127]\n",
      " [0.28718183]\n",
      " [0.30574408]\n",
      " [0.7234049 ]\n",
      " [0.30136577]\n",
      " [0.60256873]\n",
      " [0.08108224]\n",
      " [0.42919782]\n",
      " [0.79379676]\n",
      " [0.5490156 ]\n",
      " [0.69604221]\n",
      " [0.03670088]\n",
      " [0.92669153]\n",
      " [0.2044386 ]\n",
      " [0.92576061]\n",
      " [0.34726032]\n",
      " [0.12036504]\n",
      " [0.40691797]\n",
      " [0.92566398]\n",
      " [0.8469623 ]\n",
      " [0.73724138]\n",
      " [0.24309767]\n",
      " [0.058216  ]\n",
      " [0.47377404]\n",
      " [0.96869281]\n",
      " [0.19863594]\n",
      " [0.85030231]\n",
      " [0.44825668]\n",
      " [0.49313405]\n",
      " [0.63655575]\n",
      " [0.4579314 ]\n",
      " [0.80337166]\n",
      " [0.50054596]\n",
      " [0.78505977]\n",
      " [0.58878291]\n",
      " [0.95976393]\n",
      " [0.36181498]\n",
      " [0.91886391]\n",
      " [0.6891091 ]\n",
      " [0.6787517 ]\n",
      " [0.79201201]\n",
      " [0.4693551 ]\n",
      " [0.33603296]\n",
      " [0.93186124]\n",
      " [0.08907638]\n",
      " [0.45543723]\n",
      " [0.85151821]\n",
      " [0.34808815]\n",
      " [0.32102142]\n",
      " [0.07009835]\n",
      " [0.03362056]\n",
      " [0.60284801]\n",
      " [0.32416284]\n",
      " [0.67396381]\n",
      " [0.90849333]\n",
      " [0.00660163]\n",
      " [0.01356489]\n",
      " [0.29448663]\n",
      " [0.76648865]\n",
      " [0.54734252]\n",
      " [0.19032889]\n",
      " [0.61415607]\n",
      " [0.88311911]\n",
      " [0.58189399]\n",
      " [0.38891028]\n",
      " [0.37842485]\n",
      " [0.25398771]\n",
      " [0.51377026]\n",
      " [0.33876385]\n",
      " [0.6955674 ]\n",
      " [0.40963873]\n",
      " [0.07076008]\n",
      " [0.10945475]\n",
      " [0.45659391]\n",
      " [0.74840697]\n",
      " [0.14381746]\n",
      " [0.77735289]\n",
      " [0.93688407]\n",
      " [0.11242652]\n",
      " [0.64315779]\n",
      " [0.5842068 ]\n",
      " [0.71526984]\n",
      " [0.04021384]\n",
      " [0.11970694]\n",
      " [0.4302725 ]\n",
      " [0.18455126]\n",
      " [0.45557088]\n",
      " [0.25764003]\n",
      " [0.49689254]\n",
      " [0.04094545]\n",
      " [0.48561021]\n",
      " [0.36675102]\n",
      " [0.65989741]\n",
      " [0.75307137]\n",
      " [0.52033693]\n",
      " [0.59142654]\n",
      " [0.73014623]] [[0.80961347 0.60368468 0.63430245 0.64875397 0.84698402 0.3172476\n",
      "  0.96754469 0.09494267 0.8683916  0.97652586 0.53677958 0.50457237\n",
      "  0.5067841  0.43120704 0.33904766 1.01641205 0.59793868 0.87289532\n",
      "  0.49584666 0.84391735 1.01795034 0.46490608 0.86195378 0.1161407\n",
      "  0.90075302 0.43322748 0.88601121 0.66090151 0.0772639  0.17052158\n",
      "  0.69435505 0.25795905 0.18726399 0.23722299 0.27290419 0.64738535\n",
      "  0.16677284 0.1652524  0.2945002  0.83032973 0.26996614 0.44106283\n",
      "  0.52423921 0.2622807  0.85941332 0.06183959 0.13255136 0.32169587\n",
      "  1.00074777 0.32483745]]\n"
     ]
    }
   ],
   "source": [
    "# Output the reconstructed values of U and V\n",
    "\n",
    "print(U_factorized,V_factorized.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE_missing: 2.1188915087976775e-15\n",
      "RMSE_U_missing: 0.016151981573954834\n",
      "RMSE_V_missing: 0.016973077671913125\n"
     ]
    }
   ],
   "source": [
    "# Compute the RMSE between the original matrix R and the reconstructed matrix R_reconstructed\n",
    "\n",
    "RMSE_missing = np.sqrt(np.mean((R_f - R_reconstructed_missing)**2))\n",
    "\n",
    "print(\"RMSE_missing:\", RMSE_missing)\n",
    "\n",
    "# Compute the RMSE of U and V\n",
    "\n",
    "RMSE_U_missing = np.sqrt(np.mean((u - U_factorized)**2))\n",
    "\n",
    "RMSE_V_missing = np.sqrt(np.mean((v - V_factorized)**2))\n",
    "\n",
    "print(\"RMSE_U_missing:\", RMSE_U_missing)\n",
    "print(\"RMSE_V_missing:\", RMSE_V_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Missing Proportion: 0.1 ********\n",
      "RMSE_missing: 2.4052637664011314e-15\n",
      "RMSE_U_missing: 0.002903838579411324\n",
      "RMSE_V_missing: 0.002980951874122636\n",
      "******* Missing Proportion: 0.2 ********\n",
      "RMSE_missing: 2.9979264203027447e-15\n",
      "RMSE_U_missing: 0.02272930551813848\n",
      "RMSE_V_missing: 0.02416856307586616\n",
      "******* Missing Proportion: 0.3 ********\n",
      "RMSE_missing: 2.0339542859740246e-15\n",
      "RMSE_U_missing: 0.019673670629673425\n",
      "RMSE_V_missing: 0.020804598065549313\n",
      "******* Missing Proportion: 0.4 ********\n",
      "RMSE_missing: 2.5091924606756215e-15\n",
      "RMSE_U_missing: 0.032227756013432245\n",
      "RMSE_V_missing: 0.034866752088528304\n",
      "******* Missing Proportion: 0.5 ********\n",
      "RMSE_missing: 2.0882499147952527e-15\n",
      "RMSE_U_missing: 0.008849151535654218\n",
      "RMSE_V_missing: 0.009179326085041645\n",
      "******* Missing Proportion: 0.6 ********\n",
      "RMSE_missing: 5.634315082766128e-13\n",
      "RMSE_U_missing: 0.004462408611580909\n",
      "RMSE_V_missing: 0.004593396604888851\n",
      "******* Missing Proportion: 0.7 ********\n",
      "RMSE_missing: 3.241459235943753e-08\n",
      "RMSE_U_missing: 0.011058336585723885\n",
      "RMSE_V_missing: 0.01108214058241128\n",
      "******* Missing Proportion: 0.8 ********\n",
      "RMSE_missing: 0.004721950448308407\n",
      "RMSE_U_missing: 0.019795785276497526\n",
      "RMSE_V_missing: 0.0187035809231224\n",
      "******* Missing Proportion: 0.9 ********\n",
      "RMSE_missing: 0.0329380866632767\n",
      "RMSE_U_missing: 0.05879055946297784\n",
      "RMSE_V_missing: 0.017134909186473188\n"
     ]
    }
   ],
   "source": [
    "# Discuss other missing proportions for data generation and discuss their impacts in the reconstruction process\n",
    "\n",
    "# missing protions from 0.1 to 0.9\n",
    "\n",
    "missing_proportions = [0.1*i for i in range(1,10)]\n",
    "\n",
    "for missing_proportion in missing_proportions:\n",
    "\n",
    "    num_missing = int(missing_proportion * total_elements)\n",
    "\n",
    "    missing_indices = np.unravel_index(\n",
    "        np.random.choice(total_elements, num_missing, replace=False), (N, M)\n",
    "    )\n",
    "\n",
    "    R_missing = np.copy(R_f)\n",
    "    \n",
    "    R_missing[missing_indices] = np.nan\n",
    "\n",
    "    U_factorized,V_factorized = SGD_factorization(0.01, 0,1000, R_missing,False,1)\n",
    "\n",
    "    R_reconstructed_missing = U_factorized @ V_factorized.T\n",
    "\n",
    "    RMSE_missing = np.sqrt(np.mean((R_f - R_reconstructed_missing)**2))\n",
    "\n",
    "    print(\"******* Missing Proportion:\", round(missing_proportion,1),\"********\")\n",
    "    print(\"RMSE_missing:\", RMSE_missing)\n",
    "\n",
    "    RMSE_U_missing = np.sqrt(np.mean((u - U_factorized)**2))\n",
    "\n",
    "    RMSE_V_missing = np.sqrt(np.mean((v - V_factorized)**2))\n",
    "\n",
    "    print(\"RMSE_U_missing:\", RMSE_U_missing)\n",
    "\n",
    "    print(\"RMSE_V_missing:\", RMSE_V_missing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: \n",
    "\n",
    "**Reconstructed Matrix**:  \n",
    "1. For the low and moderate missing value proportion such as 10% - 50%, the RMSE is very low, indicating that the factorization can approximate the matrix accuractely. \n",
    "2. For the moderate and high missing proportion such as 60% - 80%, the RMSE increases as the missing proportion increases, showing that a slight degradation in the matrix reconstruction quality. And the degradation becomes notable as the missing proportion reaches 80%.It shows that matrix factorization methods are resilient to a moderate amount of missing data.\n",
    "3. For the very high missing proportion such as 90%, the RMSE rises significantly. The factorization struggles to approximate the original matrix since the data in the matrix becomes pretty sparse, and there is insufficient data to capture the underlying structure effectively.\n",
    "\n",
    "**Reconstructed U and V**:\n",
    "1. For the low missing proportions such as 10% - 30%, the RMSEs for U and V are very low at these missing proportions, indicating that the latent factors U and V are accurately estimated.  \n",
    "2. For the moderate missing proportions such as 40% - 60%, the RMSEs remains very low too at these missing proportions, indicating that the latent factors U and V can be accurately estimated.    \n",
    "3. For the high missing proportions such as 70% - 90%, the RMSEs for U and V becomes relatively high. It shows that for the sparse matrix, the latent factors are difficult to be estimated because the information becomes less and less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Singular Number: 1 ********\n",
      "RMSE_reconstruction: 0.1785662344904074\n",
      "******* Singular Number: 2 ********\n",
      "RMSE_reconstruction: 0.16612216880883932\n",
      "******* Singular Number: 3 ********\n",
      "RMSE_reconstruction: 0.1537641110171347\n",
      "******* Singular Number: 4 ********\n",
      "RMSE_reconstruction: 0.1405963076679247\n",
      "******* Singular Number: 5 ********\n",
      "RMSE_reconstruction: 0.12698460343689363\n",
      "******* Singular Number: 6 ********\n",
      "RMSE_reconstruction: 0.112475592395448\n",
      "******* Singular Number: 7 ********\n",
      "RMSE_reconstruction: 0.09641838014146753\n",
      "******* Singular Number: 8 ********\n",
      "RMSE_reconstruction: 0.07733966356176836\n",
      "******* Singular Number: 9 ********\n",
      "RMSE_reconstruction: 0.05442717199982621\n",
      "******* Singular Number: 10 ********\n",
      "RMSE_reconstruction: 2.460319285765109e-15\n",
      "******* Singular Number: 20 ********\n",
      "RMSE_reconstruction: 2.4639862668001584e-15\n",
      "******* Singular Number: 30 ********\n",
      "RMSE_reconstruction: 2.4676066743274875e-15\n",
      "******* Singular Number: 40 ********\n",
      "RMSE_reconstruction: 2.4701214049155486e-15\n",
      "******* Singular Number: 50 ********\n",
      "RMSE_reconstruction: 2.473262744563341e-15\n"
     ]
    }
   ],
   "source": [
    "# Generate Matrix with Rank 10\n",
    "\n",
    "N,M = 100,50\n",
    "\n",
    "full_rank_matrix = np.random.rand(N, M)\n",
    "    \n",
    "# Perform SVD decomposition\n",
    "U_10, s_10, Vt_10 = np.linalg.svd(full_rank_matrix, full_matrices=False)\n",
    "\n",
    "# Keep only the top `rank` singular values\n",
    "s_10[10:] = 0\n",
    "    \n",
    "# Reconstruct the matrix\n",
    "\n",
    "R_10 = U_10 @ np.diag(s_10) @ Vt_10\n",
    "\n",
    "# choose a moderate missing data proportions\n",
    "num_missing = int(0.3 * total_elements)\n",
    "\n",
    "missing_indices = np.unravel_index(\n",
    "    np.random.choice(total_elements, num_missing, replace=False), (N, M)\n",
    ")\n",
    "\n",
    "R_missing_10 = np.copy(R_10)\n",
    "\n",
    "R_missing_10[missing_indices] = np.nan\n",
    "\n",
    "# decompose the matrix using different number of singular components\n",
    "\n",
    "singular_components = [1,2,3,4,5,6,7,8,9,10, 20, 30, 40, 50]\n",
    "\n",
    "for K in singular_components:\n",
    "\n",
    "    U_reconstructed, s_constructed, V_reconstructed = np.linalg.svd(R_10, full_matrices=False)\n",
    "    \n",
    "    s_constructed[K:] = 0\n",
    "\n",
    "    R_reconstructed_k = U_reconstructed @ np.diag(s_constructed) @ V_reconstructed\n",
    "\n",
    "    RMSE_missing = np.sqrt(np.mean((R_10 - R_reconstructed_k)**2))\n",
    "\n",
    "    print(\"******* Singular Number:\", K ,\"********\")\n",
    "    print(\"RMSE_reconstruction:\", RMSE_missing)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**:   \n",
    "I use the a rank-10 matrix with 30% missing values.\n",
    "1. For the number of singular value smaller than or equal to the Rank of the matrix. As the number of singular value increases, the reconstruction effect increases because we use more features. And the MSE becomes very small as the number of singualr value is equal to the Rank of the matrix.\n",
    "2. For the number of singular values bigger than the Rank of the matrix. As the number of singular value increases, the reconstruction effect remains stable because singular values after the 10th are quite small so there is very little information in those singular components of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
