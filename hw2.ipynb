{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 SVD for a Noisy Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix Shape: (100, 50)\n"
     ]
    }
   ],
   "source": [
    "# Generate the matrix\n",
    "\n",
    "N,M = 100,50\n",
    "\n",
    "# Form 2 random vectors\n",
    "u=np.random.rand(N,1)\n",
    "v=np.random.rand(M,1)\n",
    "\n",
    "# Compute the rank-1 matrix using outer product\n",
    "R_1 = u @ v.T\n",
    "\n",
    "# Calculate the Frobenius norm of this matrix\n",
    "R_f = np.linalg.norm(R_1, 'fro')\n",
    "\n",
    "noise_variance = 0.01 * R_f\n",
    "\n",
    "noise = np.random.normal(0, np.sqrt(noise_variance), (N,M)) \n",
    "\n",
    "R_ = R_1 + noise\n",
    "\n",
    "# Display the shape of the matrix\n",
    "\n",
    "print(\"The matrix Shape:\", R_.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U Shape: (100, 50)\n",
      "s Shape: (50,)\n",
      "V Shape: (50, 50)\n"
     ]
    }
   ],
   "source": [
    "# Decompose the matrix using SVD\n",
    "\n",
    "U1, s1, V1 = np.linalg.svd(R_, full_matrices=False)\n",
    "\n",
    "print(\"U Shape:\", U1.shape)\n",
    "\n",
    "print(\"s Shape:\", s1.shape)\n",
    "\n",
    "print(\"V Shape:\", V1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the matrix R using only the first singular value and the corresponidng singular vectors\n",
    "\n",
    "R_reconstructed = s1[0] * np.outer(U1[:,0], V1[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U_original Shape: [[-0.08649708  0.5078826   0.01117663 ...  0.00453865 -0.00879669\n",
      "  -0.85328956]\n",
      " [-0.05187844  0.01301662 -0.06115115 ... -0.02396857  0.08277716\n",
      "  -0.00129157]\n",
      " [-0.07324134  0.16436727 -0.10248218 ... -0.02820926  0.01838645\n",
      "   0.09995934]\n",
      " ...\n",
      " [-0.15586783 -0.10076451 -0.17997936 ...  0.11908958  0.17305308\n",
      "  -0.05217211]\n",
      " [-0.11281654  0.01832965 -0.02631149 ... -0.01959697  0.02359228\n",
      "   0.00892189]\n",
      " [-0.01162413 -0.00262869  0.01495646 ...  0.03721343 -0.01408885\n",
      "   0.0020473 ]]\n",
      "s_original Shape: [2.43977432e+01 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 2.40840268e-15 2.40840268e-15 2.40840268e-15\n",
      " 2.40840268e-15 4.79774828e-16]\n",
      "V_original Shape: [[-1.54893252e-01 -6.81490513e-02 -1.08470348e-01 ... -1.30434130e-01\n",
      "  -3.76970233e-02 -1.50490446e-01]\n",
      " [ 0.00000000e+00 -1.27118780e-01 -2.92986379e-02 ... -7.86784304e-02\n",
      "  -3.34973307e-02  1.39199903e-01]\n",
      " [ 0.00000000e+00  7.60147670e-02  2.19941279e-01 ... -2.47597974e-03\n",
      "  -1.31703582e-02  3.65030723e-01]\n",
      " ...\n",
      " [ 0.00000000e+00  2.77787426e-02  2.29757164e-02 ...  1.32255020e-02\n",
      "   3.48797414e-03  9.35265911e-03]\n",
      " [ 0.00000000e+00 -6.63952632e-02 -3.37505306e-02 ... -6.05445040e-04\n",
      "  -9.53024455e-02 -1.62151456e-02]\n",
      " [ 9.87931212e-01 -1.06847805e-02 -1.70065736e-02 ... -2.04501754e-02\n",
      "  -5.91034523e-03 -2.35947142e-02]]\n"
     ]
    }
   ],
   "source": [
    "# calculate and output the original values of U and V\n",
    "\n",
    "U_orginal, s_original, V_original = np.linalg.svd(R_1, full_matrices=False)\n",
    "\n",
    "print(\"U_original Shape:\", U_orginal)\n",
    "\n",
    "print(\"s_original Shape:\", s_original)\n",
    "\n",
    "print(\"V_original Shape:\", V_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed U: [[-0.06204062 -0.88157473 -0.01035053 ... -0.00953932  0.01971828\n",
      "  -0.45416763]\n",
      " [-0.09627897  0.0603207   0.10790332 ... -0.10848051  0.00869856\n",
      "  -0.10803063]\n",
      " [-0.02815318  0.02002511  0.05553716 ...  0.01209248 -0.0309908\n",
      "  -0.0272092 ]\n",
      " ...\n",
      " [-0.16886511 -0.01956972  0.32818846 ... -0.02846281  0.05145675\n",
      "   0.08269326]\n",
      " [-0.10713838  0.07971953  0.00547247 ...  0.11783544 -0.09092396\n",
      "  -0.12263087]\n",
      " [-0.02152564  0.0062851   0.00696212 ... -0.0379119  -0.04339649\n",
      "  -0.00676242]]\n",
      "Reconstructed V: [[-0.15172915 -0.05269024 -0.10478298 ... -0.11008482 -0.06096405\n",
      "  -0.15704673]\n",
      " [-0.2322151   0.00342671  0.05747582 ... -0.06760861  0.04971445\n",
      "  -0.04530962]\n",
      " [ 0.          0.04457724 -0.11732008 ... -0.13479449  0.08802128\n",
      "   0.07288781]\n",
      " ...\n",
      " [ 0.          0.07871962  0.05372124 ... -0.01324373  0.0458514\n",
      "  -0.01221011]\n",
      " [ 0.          0.04626426  0.02714949 ... -0.04483317  0.09693942\n",
      "  -0.02796269]\n",
      " [-0.96075721  0.00749296  0.00265611 ...  0.03372633 -0.00238814\n",
      "   0.0357532 ]]\n",
      "Reconstructed s: [2.51199819e+01 3.67108916e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 2.48093001e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 2.48093001e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 2.48093001e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 2.48093001e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 2.48093001e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 2.48093001e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 2.48093001e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 2.48093001e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 2.48093001e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 2.48093001e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 2.48093001e-15 2.48093001e-15 2.48093001e-15\n",
      " 2.48093001e-15 7.73032180e-16]\n"
     ]
    }
   ],
   "source": [
    "# calculate and output the Reconstructed values of U and V\n",
    "\n",
    "U_reconstructed, s_constructed, V_reconstructed = np.linalg.svd(R_reconstructed, full_matrices=False)\n",
    "\n",
    "print(\"Reconstructed U:\", U_reconstructed)\n",
    "\n",
    "print(\"Reconstructed V:\", V_reconstructed)\n",
    "\n",
    "print(\"Reconstructed s:\", s_constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.08489810774537972\n",
      "RMSE_U: 0.14178106092962273\n",
      "RMSE_V: 0.19777987067223007\n"
     ]
    }
   ],
   "source": [
    "# Computer the Root Mean Squared Error between the original matrix R and the reconstructed matrix R_reconstructed \n",
    "\n",
    "RMSE = np.sqrt(np.mean((R_1 - R_reconstructed)**2))\n",
    "\n",
    "print(\"RMSE:\", RMSE)\n",
    "\n",
    "# Compute the Root Mean Squared Errors of U and V \n",
    "# From the value of U_original, V_original and U_reconstructed, V_reconstructed, there is no need to adjust the sign of the singular vectors when calculating the RMSE\n",
    "\n",
    "RMSE_U = np.sqrt(np.mean((U_orginal - U_reconstructed)**2))\n",
    "\n",
    "RMSE_V = np.sqrt(np.mean((V_original- V_reconstructed)**2))\n",
    "\n",
    "print(\"RMSE_U:\", RMSE_U)\n",
    "print(\"RMSE_V:\", RMSE_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The impact of noise:**\n",
    "\n",
    "1. Effect on the Matrix Reconstruction: The RMSE for the reconstructed matrix is pretty small, indicating that the reconstruction closely approximates the original matrix despite the added noise. This is potentially because the reconstruction uses only the first singular value and the corresponding singular vectors, which contains most information of the matrix. This filters out higher-order singular values, which captures a portion of the noise, reducing the effect of noise on the reconstructed matrix.  \n",
    "2. Effect on the Singular Vectors: The RMSE for the singular vectors suggests that the noise has introduced some discrepancy between the original and reconstructed singular vectors. Singular vectors associated with significant singular values are typically less affected by noise because they represent more prominent structural components of the matrix. But the loss indicates that noise might alter the direction slightly, resulting the overall discrepancies. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Matrix Factorization of an Imcomplete Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Matrix \n",
    "\n",
    "# Calculate the total number of elements and 30% of that\n",
    "total_elements = N * M\n",
    "num_missing = int(0.3 * total_elements)\n",
    "\n",
    "# Randomly select indices to set as missing\n",
    "missing_indices = np.unravel_index(\n",
    "    np.random.choice(total_elements, num_missing, replace=False), (N, M)\n",
    ")\n",
    "\n",
    "# Create a copy of the original matrix and set the missing values to nan\n",
    "\n",
    "R_missing = np.copy(R_1)\n",
    "\n",
    "R_missing[missing_indices] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 246.8775189472206\n",
      "Epoch: 10 Loss: 20.8793028311295\n",
      "Epoch: 20 Loss: 1.7271472356901725\n",
      "Epoch: 30 Loss: 0.1656142820981253\n",
      "Epoch: 40 Loss: 0.01724478266299418\n",
      "Epoch: 50 Loss: 0.0019221695950491563\n",
      "Epoch: 60 Loss: 0.00022781361739832446\n",
      "Epoch: 70 Loss: 2.85444776194914e-05\n",
      "Epoch: 80 Loss: 3.7605024899771773e-06\n",
      "Epoch: 90 Loss: 5.181445682039893e-07\n",
      "Epoch: 100 Loss: 7.428391338065902e-08\n",
      "Epoch: 110 Loss: 1.1025660647903143e-08\n",
      "Epoch: 120 Loss: 1.6862060519430864e-09\n",
      "Epoch: 130 Loss: 2.645369909142207e-10\n",
      "Epoch: 140 Loss: 4.2401766674303685e-11\n",
      "Epoch: 150 Loss: 6.919195770328155e-12\n",
      "Epoch: 160 Loss: 1.1459370251412356e-12\n",
      "Epoch: 170 Loss: 1.9211412683150032e-13\n",
      "Epoch: 180 Loss: 3.25309132977319e-14\n",
      "Epoch: 190 Loss: 5.5536808980421015e-15\n"
     ]
    }
   ],
   "source": [
    "# SGD for matrix factorization\n",
    "\n",
    "def SGD_factorization(learning_rate, regularization, num_epochs, R_missing, output):\n",
    "\n",
    "    U_factorized = np.random.rand(N,1)\n",
    "    V_factorized = np.random.rand(M,1) # initialze the U and V matrices\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for i in range(N):\n",
    "            for j in range(M):\n",
    "                \n",
    "                # Only consider the observed values\n",
    "                if not np.isnan(R_missing[i,j]):\n",
    "\n",
    "                    prediction = np.dot(U_factorized[i], V_factorized[j])\n",
    "                    error = R_missing[i,j] - prediction\n",
    "\n",
    "                    # Update the U and V matrices\n",
    "                    U_factorized[i] += learning_rate * (error * V_factorized[j] - regularization * U_factorized[i])\n",
    "                    V_factorized[j] += learning_rate * (error * U_factorized[i] - regularization * V_factorized[j]) \n",
    "                    \n",
    "        if output:\n",
    "            if epoch % 10 == 0:\n",
    "            \n",
    "                # Calculatre the total loss on observed entrices\n",
    "                observed_indices = ~np.isnan(R_missing)\n",
    "                loss = np.sum((R_missing[observed_indices] - (U_factorized @ V_factorized.T)[observed_indices])**2)\n",
    "                print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "    \n",
    "    return U_factorized, V_factorized\n",
    "\n",
    "U_factorized,V_factorized = SGD_factorization(0.01, 0, 200, R_missing,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the factorized matrices U and V to reconstruct the matrix R\n",
    "\n",
    "R_reconstructed_missing = U_factorized @ V_factorized.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE_missing: 6.856891714722553e-10\n"
     ]
    }
   ],
   "source": [
    "# Compute the RMSE between the original matrix R and the reconstructed matrix R_reconstructed\n",
    "\n",
    "RMSE_missing = np.sqrt(np.mean((R_1 - R_reconstructed_missing)**2))\n",
    "\n",
    "print(\"RMSE_missing:\", RMSE_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Missing Proportion: 0.1 ********\n",
      "RMSE_missing: 3.0146844803622937e-15\n",
      "******* Missing Proportion: 0.2 ********\n",
      "RMSE_missing: 2.5600596123022374e-15\n",
      "******* Missing Proportion: 0.3 ********\n",
      "RMSE_missing: 2.1857767505743337e-15\n",
      "******* Missing Proportion: 0.4 ********\n",
      "RMSE_missing: 1.879971250281595e-15\n",
      "******* Missing Proportion: 0.5 ********\n",
      "RMSE_missing: 2.4484703635746526e-15\n",
      "******* Missing Proportion: 0.6 ********\n",
      "RMSE_missing: 2.7660834987431875e-15\n",
      "******* Missing Proportion: 0.7 ********\n",
      "RMSE_missing: 3.1426527736688745e-10\n",
      "******* Missing Proportion: 0.8 ********\n",
      "RMSE_missing: 7.91531367972342e-05\n",
      "******* Missing Proportion: 0.9 ********\n",
      "RMSE_missing: 0.011778686221995027\n",
      "******* Missing Proportion: 0.95 ********\n",
      "RMSE_missing: 0.10647241699226816\n",
      "******* Missing Proportion: 0.97 ********\n",
      "RMSE_missing: 0.15001416504053208\n",
      "******* Missing Proportion: 0.99 ********\n",
      "RMSE_missing: 0.2302076053784779\n"
     ]
    }
   ],
   "source": [
    "# Discuss other missing proportions for data generation and discuss their impacts in the reconstruction process\n",
    "\n",
    "# missing protions from 0.1 to 0.9\n",
    "\n",
    "missing_proportions = [0.1*i for i in range(1,10)]\n",
    "\n",
    "missing_proportions.extend([0.95,0.97,0.99])\n",
    "\n",
    "for missing_proportion in missing_proportions:\n",
    "\n",
    "    num_missing = int(missing_proportion * total_elements)\n",
    "\n",
    "    missing_indices = np.unravel_index(\n",
    "        np.random.choice(total_elements, num_missing, replace=False), (N, M)\n",
    "    )\n",
    "\n",
    "    R_missing = np.copy(R_1)\n",
    "    \n",
    "    R_missing[missing_indices] = np.nan\n",
    "\n",
    "    U_factorized,V_factorized = SGD_factorization(0.01, 0,1000, R_missing,False)\n",
    "\n",
    "    R_reconstructed_missing = U_factorized @ V_factorized.T\n",
    "\n",
    "    RMSE_missing = np.sqrt(np.mean((R_1 - R_reconstructed_missing)**2))\n",
    "\n",
    "    print(\"******* Missing Proportion:\", round(missing_proportion,2),\"********\")\n",
    "    print(\"RMSE_missing:\", RMSE_missing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion impact of the missing value proportion on reconstructed matrix**: \n",
    "\n",
    "1. For the low and moderate missing value proportion such as 10% - 70%, the RMSE is very low, indicating that the factorization can approximate the missing value and the observed value accurately.  \n",
    "2. For the very high missing proportion such as 80% - 99%, the RMSE continues to increase. For the missing value proportion above 90%, the discrepancy of original and reconstructed matrix is notable, indicating the matrix factorization struggles to impute the missing value of the matrix when the matrix is extremely sparse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Singular Number: 1 ********\n",
      "RMSE_reconstruction: 0.17942279666318597\n",
      "******* Singular Number: 2 ********\n",
      "RMSE_reconstruction: 0.1662126903557679\n",
      "******* Singular Number: 3 ********\n",
      "RMSE_reconstruction: 0.15369056682766777\n",
      "******* Singular Number: 4 ********\n",
      "RMSE_reconstruction: 0.1403840365385495\n",
      "******* Singular Number: 5 ********\n",
      "RMSE_reconstruction: 0.12667487566391875\n",
      "******* Singular Number: 6 ********\n",
      "RMSE_reconstruction: 0.11196449985081135\n",
      "******* Singular Number: 7 ********\n",
      "RMSE_reconstruction: 0.09623198057062363\n",
      "******* Singular Number: 8 ********\n",
      "RMSE_reconstruction: 0.0777436333897794\n",
      "******* Singular Number: 9 ********\n",
      "RMSE_reconstruction: 0.05461470473860565\n",
      "******* Singular Number: 10 ********\n",
      "RMSE_reconstruction: 6.705351340375311e-16\n",
      "******* Singular Number: 20 ********\n",
      "RMSE_reconstruction: 6.824197241619624e-16\n",
      "******* Singular Number: 30 ********\n",
      "RMSE_reconstruction: 6.939150368551093e-16\n",
      "******* Singular Number: 40 ********\n",
      "RMSE_reconstruction: 7.048432124738406e-16\n",
      "******* Singular Number: 50 ********\n",
      "RMSE_reconstruction: 7.16858702762589e-16\n"
     ]
    }
   ],
   "source": [
    "# Generate Matrix with Rank 10\n",
    "\n",
    "N,M = 100,50\n",
    "\n",
    "full_rank_matrix = np.random.rand(N, M)\n",
    "    \n",
    "# Perform SVD decomposition\n",
    "U_10, s_10, Vt_10 = np.linalg.svd(full_rank_matrix, full_matrices=False)\n",
    "\n",
    "# Keep only the top `rank` singular values\n",
    "s_10[10:] = 0\n",
    "    \n",
    "# Reconstruct the matrix\n",
    "\n",
    "R_10 = U_10 @ np.diag(s_10) @ Vt_10\n",
    "\n",
    "# choose a moderate missing data proportions\n",
    "num_missing = int(0.3 * total_elements)\n",
    "\n",
    "missing_indices = np.unravel_index(\n",
    "    np.random.choice(total_elements, num_missing, replace=False), (N, M)\n",
    ")\n",
    "\n",
    "R_missing_10 = np.copy(R_10)\n",
    "\n",
    "R_missing_10[missing_indices] = np.nan\n",
    "\n",
    "# decompose the matrix using different number of singular components\n",
    "\n",
    "singular_components = [1,2,3,4,5,6,7,8,9,10, 20, 30, 40, 50]\n",
    "\n",
    "for K in singular_components:\n",
    "\n",
    "    U_reconstructed, s_constructed, V_reconstructed = np.linalg.svd(R_10, full_matrices=False)\n",
    "    \n",
    "    s_constructed[K:] = 0\n",
    "\n",
    "    R_reconstructed_k = U_reconstructed @ np.diag(s_constructed) @ V_reconstructed\n",
    "\n",
    "    RMSE_missing = np.sqrt(np.mean((R_10 - R_reconstructed_k)**2))\n",
    "\n",
    "    print(\"******* Singular Number:\", K ,\"********\")\n",
    "    print(\"RMSE_reconstruction:\", RMSE_missing)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**:   \n",
    "I use the a rank-10 matrix with 30% missing values.\n",
    "1. For the number of singular components smaller than or equal to the Rank of the matrix. As the number of singular components increases, the reconstruction effect increases because for the kth singular compoents (k<=10), the singular components contain part information of the matrix. And the MSE becomes very small as the number of singualr components is equal to the Rank of the matrix because we use nearly all useful information to reconstruct the matrix.  \n",
    "2. For the number of singular values bigger than the Rank of the matrix. As the number of singular value increases, the reconstruction effect remains stable because singular values after the 10th are quite small so there is little information in those singular components of the matrix. So there is no improvements for reconstruction using k th singular components (k>10).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Matrix Factorization with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of elements and 30% of that\n",
    "total_elements = N * M\n",
    "\n",
    "num_missing = int(0.8 * total_elements)\n",
    "\n",
    "# Randomly select indices to set as missing\n",
    "missing_indices = np.unravel_index(\n",
    "    np.random.choice(total_elements, num_missing, replace=False), (N, M)\n",
    ")\n",
    "\n",
    "# Create a copy of the original matrix and set the missing values to nan\n",
    "\n",
    "R_missing = np.copy(R_1)\n",
    "\n",
    "R_missing[missing_indices] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4000)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(R_missing).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank-one factorization using regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_reconstrcuion [[0.32687131 0.14381702 0.22890687 ... 0.27526085 0.07955473 0.31757722]\n",
      " [0.19604535 0.08625614 0.13728989 ... 0.1650913  0.04771399 0.19047109]\n",
      " [0.27677472 0.12177549 0.19382439 ... 0.23307412 0.0673621  0.26890504]\n",
      " ...\n",
      " [0.58900421 0.25915039 0.41247764 ... 0.49600497 0.14335326 0.57225675]\n",
      " [0.42631139 0.18756871 0.29854442 ... 0.3590001  0.10375669 0.41418987]\n",
      " [0.0439268  0.01932694 0.03076179 ... 0.0369911  0.01069101 0.04267781]]\n",
      "R_original [[0.32687641 0.14381722 0.22890861 ... 0.27525951 0.07955329 0.31758503]\n",
      " [0.19605098 0.0862574  0.13729274 ... 0.16509266 0.04771375 0.19047828]\n",
      " [0.27678239 0.12177714 0.19382821 ... 0.23307581 0.0673617  0.26891492]\n",
      " ...\n",
      " [0.58903166 0.2591588  0.41249356 ... 0.49601794 0.14335512 0.57228856]\n",
      " [0.42633888 0.18757815 0.29856128 ... 0.35901591 0.10375989 0.41422029]\n",
      " [0.04392812 0.01932724 0.03076246 ... 0.03699145 0.01069097 0.04267947]]\n",
      "RMSE_missing: 1.9990277827520853e-05\n"
     ]
    }
   ],
   "source": [
    "# The result of Rank-one factorization using regularization\n",
    "\n",
    "# set regularization to 0.1 as an example\n",
    "\n",
    "U_factorized,V_factorized = SGD_factorization(0.01,1e-5, 1000, R_missing,False)\n",
    "\n",
    "R_reconstructed_missing = U_factorized @ V_factorized.T\n",
    "\n",
    "RMSE_missing = np.sqrt(np.mean((R_1 - R_reconstructed_missing)**2))\n",
    "\n",
    "print(\"R_reconstrcuion\", R_reconstructed_missing)\n",
    "\n",
    "print(\"R_original\",R_1)\n",
    "\n",
    "print(\"RMSE_missing:\", RMSE_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion the selection of lamda on the reconstruction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Regularization: 0.0 ********\n",
      "RMSE_missing: 1.7295401085124914e-05\n",
      "******* Regularization: 1e-06 ********\n",
      "RMSE_missing: 3.970605958899954e-05\n",
      "******* Regularization: 2e-06 ********\n",
      "RMSE_missing: 1.1413570757640905e-05\n",
      "******* Regularization: 3e-06 ********\n",
      "RMSE_missing: 3.053911236677769e-05\n",
      "******* Regularization: 4e-06 ********\n",
      "RMSE_missing: 1.0385992449091427e-05\n",
      "******* Regularization: 4.9999999999999996e-06 ********\n",
      "RMSE_missing: 1.084148953503658e-05\n",
      "******* Regularization: 6e-06 ********\n",
      "RMSE_missing: 4.53837481536864e-05\n",
      "******* Regularization: 7e-06 ********\n",
      "RMSE_missing: 3.914196535243085e-05\n",
      "******* Regularization: 8e-06 ********\n",
      "RMSE_missing: 3.8189555074555896e-05\n",
      "******* Regularization: 9e-06 ********\n",
      "RMSE_missing: 1.6332794224055282e-05\n",
      "******* Regularization: 9.999999999999999e-06 ********\n",
      "RMSE_missing: 6.885195566661244e-05\n"
     ]
    }
   ],
   "source": [
    "lamda_all= [1e-6* i for i in range(0,11)]\n",
    "\n",
    "for lamda in lamda_all:\n",
    "    \n",
    "    U_factorized,V_factorized = SGD_factorization(0.01, lamda, 1000, R_missing,False)\n",
    "\n",
    "    R_reconstructed_missing = U_factorized @ V_factorized.T\n",
    "\n",
    "    RMSE_missing = np.sqrt(np.mean((R_1 - R_reconstructed_missing)**2))\n",
    "\n",
    "    print(\"******* Regularization:\", lamda ,\"********\")\n",
    "    print(\"RMSE_missing:\", RMSE_missing)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**:  \n",
    "I select lambda from 1e-6 to 1e-5.  \n",
    "1. When the lambda is small, the regularization term has minimal effect, allowing U and V to fit the observed entries of R more closely. This increases the risk of overfitting. When the lambda is below 4e-6, the effect of reconstruction with regularization is similar to the effect of no regularization reconstruction.  \n",
    "2. When the lambda is moderate, the regularization term could "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
