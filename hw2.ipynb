{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD for a Noisy Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix Shape: (100, 50)\n"
     ]
    }
   ],
   "source": [
    "# Generate the matrix\n",
    "\n",
    "N,M = 100,50\n",
    "\n",
    "# Form 2 random vectors\n",
    "u=np.random.rand(N,1)\n",
    "v=np.random.rand(M,1)\n",
    "\n",
    "# Compute the rank-1 matrix using outer product\n",
    "R_1 = u @ v.T\n",
    "\n",
    "# Calculate the Frobenius norm of this matrix\n",
    "R_f = np.linalg.norm(R_1, 'fro')\n",
    "\n",
    "noise_variance = 0.01 * R_f\n",
    "\n",
    "noise = np.random.normal(0, np.sqrt(noise_variance), (N,M)) \n",
    "\n",
    "R1 = R_1 + noise\n",
    "\n",
    "# Display the shape of the matrix\n",
    "\n",
    "print(\"The matrix Shape:\", R1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U Shape: (100, 50)\n",
      "s Shape: (50,)\n",
      "V Shape: (50, 50)\n"
     ]
    }
   ],
   "source": [
    "# Decompose the matrix using SVD\n",
    "\n",
    "U1, s1, V1 = np.linalg.svd(R1, full_matrices=False)\n",
    "\n",
    "print(\"U Shape:\", U1.shape)\n",
    "\n",
    "print(\"s Shape:\", s1.shape)\n",
    "\n",
    "print(\"V Shape:\", V1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the matrix R using only the first singular value and the corresponidng singular vectors\n",
    "\n",
    "R_reconstructed = s1[0] * np.outer(U1[:,0], V1[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed U: [[-1.40545577e-01 -3.88810651e-01 -1.13780532e-02 ...  3.40681904e-02\n",
      "  -1.43403874e-02 -9.00852545e-01]\n",
      " [-1.69706563e-01 -2.21097275e-01  5.92597076e-02 ...  3.37317189e-02\n",
      "   7.60568510e-02  1.30611266e-01]\n",
      " [-2.67237109e-02  2.29133000e-02 -5.73232923e-03 ...  3.63035398e-02\n",
      "  -1.25668939e-01 -2.06068320e-03]\n",
      " ...\n",
      " [-8.87169709e-03 -7.87872014e-03  2.77197068e-04 ... -1.93479828e-02\n",
      "  -5.45768519e-02  3.07122692e-03]\n",
      " [-1.36255590e-01 -1.35885062e-01  5.75929384e-02 ... -1.01260210e-01\n",
      "  -1.04762409e-01  8.13625055e-02]\n",
      " [-2.92055852e-02  1.74626444e-02  3.86346975e-02 ... -1.39546579e-01\n",
      "   5.08470049e-02 -4.19144948e-03]]\n",
      "Reconstructed V: [[-7.96681771e-03 -5.91892935e-02 -1.10278124e-01 ... -1.01730542e-01\n",
      "  -1.19060360e-01 -3.74743370e-02]\n",
      " [ 0.00000000e+00 -3.78180374e-02 -1.73965432e-01 ... -4.58704945e-03\n",
      "   1.16330169e-02 -3.12785770e-03]\n",
      " [ 0.00000000e+00  2.08108812e-02 -4.63045880e-03 ...  1.03924411e-01\n",
      "  -5.73311891e-02  5.39339566e-02]\n",
      " ...\n",
      " [ 0.00000000e+00 -3.58672281e-04 -1.01195263e-02 ...  9.31389142e-03\n",
      "  -6.03366133e-03  2.45707575e-02]\n",
      " [ 0.00000000e+00 -3.98227551e-03 -1.29168084e-02 ...  1.12404589e-02\n",
      "  -9.17875499e-03  6.40672022e-02]\n",
      " [-9.99968264e-01  4.71565278e-04  8.78593593e-04 ...  8.10494404e-04\n",
      "   9.48562288e-04  2.98560687e-04]]\n"
     ]
    }
   ],
   "source": [
    "# output the Reconstructed values of U and V\n",
    "\n",
    "U_reconstructed, s_constructed, V_reconstructed = np.linalg.svd(R_reconstructed, full_matrices=False)\n",
    "\n",
    "print(\"Reconstructed U:\", U_reconstructed)\n",
    "\n",
    "print(\"Reconstructed V:\", V_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.49264272196866243\n",
      "RMSE_U: 0.14022221684691455\n",
      "RMSE_V: 0.19550942317387476\n"
     ]
    }
   ],
   "source": [
    "# Computer the Root Mean Squared Error between the original matrix R and the reconstructed matrix R_reconstructed \n",
    "\n",
    "RMSE = np.sqrt(np.mean((R1 - R_reconstructed)**2))\n",
    "\n",
    "print(\"RMSE:\", RMSE)\n",
    "\n",
    "# Computer the Root Mean Squared Errors of U and V\n",
    "\n",
    "RMSE_U = np.sqrt(np.mean((U1 - U_reconstructed)**2))\n",
    "\n",
    "RMSE_V = np.sqrt(np.mean((V1- V_reconstructed)**2))\n",
    "\n",
    "print(\"RMSE_U:\", RMSE_U)\n",
    "print(\"RMSE_V:\", RMSE_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The impact of noise:** : \n",
    "\n",
    "1. The RMSE between the original and reconstructed R suggests that noise increases the overall discrepancy between the original noisy matrix and the reconstructed matrix. It impacts the overall rank of the matrix so just using the first singular value to construct matrix would lose some imformation of the matrix.\n",
    "2. The RMSE between the original and reconstructed U and V suggests that noise increases the overall discrepancy between the original and the reconstructed singular vectors. For the rank-1 matrix, ideally only the first singular value and corresponding singular vectors would be non-zero. However, after noising, additional vectors may become non-zero but those corrisponding singular vectors of reconstructed matrix are not zero, which impacts the overall discrepancy between the original and reconstructed singular vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization of an Imcomplete Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the Matrix \n",
    "\n",
    "# Calculate the total number of elements and 30% of that\n",
    "total_elements = N * M\n",
    "num_missing = int(0.3 * total_elements)\n",
    "\n",
    "# Randomly select indices to set as missing\n",
    "missing_indices = np.unravel_index(\n",
    "    np.random.choice(total_elements, num_missing, replace=False), (N, M)\n",
    ")\n",
    "\n",
    "# Create a copy of the original matrix and set the missing values to nan\n",
    "\n",
    "R_missing = np.copy(R_1)\n",
    "\n",
    "R_missing[missing_indices] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 262.23028902885807\n",
      "Epoch: 10 Loss: 20.437717632531182\n",
      "Epoch: 20 Loss: 1.3836255332651866\n",
      "Epoch: 30 Loss: 0.11393311358811087\n",
      "Epoch: 40 Loss: 0.010625924773048858\n",
      "Epoch: 50 Loss: 0.0010949112046464199\n",
      "Epoch: 60 Loss: 0.00012265331457071626\n",
      "Epoch: 70 Loss: 1.4733668957722918e-05\n",
      "Epoch: 80 Loss: 1.8752354329138669e-06\n",
      "Epoch: 90 Loss: 2.502176701418193e-07\n",
      "Epoch: 100 Loss: 3.46798422223587e-08\n",
      "Epoch: 110 Loss: 4.953206359071287e-09\n",
      "Epoch: 120 Loss: 7.242185822727976e-10\n",
      "Epoch: 130 Loss: 1.0781798835276882e-10\n",
      "Epoch: 140 Loss: 1.6274159289547836e-11\n",
      "Epoch: 150 Loss: 2.4822703543529292e-12\n",
      "Epoch: 160 Loss: 3.8162285999731485e-13\n",
      "Epoch: 170 Loss: 5.902155713602591e-14\n",
      "Epoch: 180 Loss: 9.169367259399356e-15\n",
      "Epoch: 190 Loss: 1.4293512867112417e-15\n"
     ]
    }
   ],
   "source": [
    "# SGD for matrix factorization\n",
    "\n",
    "def SGD_factorization(learning_rate, regularization, num_epochs, R_missing, output, Rank):\n",
    "\n",
    "    K= Rank\n",
    "    U_factorized = np.random.rand(N,K)\n",
    "    V_factorized = np.random.rand(M,K) # initialze the U and V matrices\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for i in range(N):\n",
    "            for j in range(M):\n",
    "                \n",
    "                # Only consider the observed values\n",
    "                if not np.isnan(R_missing[i,j]):\n",
    "\n",
    "                    prediction = np.dot(U_factorized[i,:], V_factorized[j,:])\n",
    "                    error = R_missing[i,j] - prediction\n",
    "\n",
    "                    # Update the U and V matrices\n",
    "                    U_factorized[i,:] += learning_rate * (error * V_factorized[j,:] - regularization * U_factorized[i,:])\n",
    "                    V_factorized[j,:] += learning_rate * (error * U_factorized[i,:] - regularization * V_factorized[j,:]) \n",
    "                    \n",
    "        if output:\n",
    "            if epoch % 10 == 0:\n",
    "            \n",
    "                # Calculatre the total loss on observed entrices\n",
    "                observed_indices = ~np.isnan(R_missing)\n",
    "                loss = np.sum((R_missing[observed_indices] - (U_factorized @ V_factorized.T)[observed_indices])**2)\n",
    "                print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "    \n",
    "    return U_factorized, V_factorized\n",
    "\n",
    "U_factorized,V_factorized = SGD_factorization(0.01, 0, 200, R_missing,True,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the factorized matrices U and V to reconstruct the matrix R\n",
    "\n",
    "R_reconstructed_missing = U_factorized @ V_factorized.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7152564 ]\n",
      " [0.81904694]\n",
      " [0.04719282]\n",
      " [0.96831662]\n",
      " [0.17734362]\n",
      " [0.85194888]\n",
      " [0.13261678]\n",
      " [0.04605048]\n",
      " [0.87004177]\n",
      " [0.61871822]\n",
      " [0.00150608]\n",
      " [0.11084721]\n",
      " [0.82864751]\n",
      " [0.85033771]\n",
      " [0.36328974]\n",
      " [0.95287898]\n",
      " [0.78230286]\n",
      " [0.16678854]\n",
      " [0.5565714 ]\n",
      " [0.30847425]\n",
      " [0.19747275]\n",
      " [0.35203172]\n",
      " [0.80234439]\n",
      " [0.37382909]\n",
      " [0.87357362]\n",
      " [0.63027812]\n",
      " [0.80451471]\n",
      " [0.88970243]\n",
      " [0.08598488]\n",
      " [0.76837801]\n",
      " [0.44208659]\n",
      " [0.13032667]\n",
      " [0.05207936]\n",
      " [0.68158423]\n",
      " [0.75143342]\n",
      " [0.75007879]\n",
      " [0.02469512]\n",
      " [0.65278551]\n",
      " [0.34505139]\n",
      " [0.29632159]\n",
      " [0.32665468]\n",
      " [0.40141623]\n",
      " [0.6508986 ]\n",
      " [0.68572236]\n",
      " [0.43057385]\n",
      " [0.12803385]\n",
      " [0.36686361]\n",
      " [0.24671721]\n",
      " [0.86357919]\n",
      " [0.52290546]\n",
      " [0.48418577]\n",
      " [0.77898762]\n",
      " [0.7527056 ]\n",
      " [0.82294716]\n",
      " [0.05592674]\n",
      " [0.16088572]\n",
      " [0.60792554]\n",
      " [0.76390685]\n",
      " [0.06456823]\n",
      " [0.21695574]\n",
      " [0.07080704]\n",
      " [0.86197101]\n",
      " [0.73382996]\n",
      " [0.76135915]\n",
      " [0.51397814]\n",
      " [0.66718528]\n",
      " [0.20844474]\n",
      " [0.02871127]\n",
      " [0.1727203 ]\n",
      " [0.40211045]\n",
      " [0.72228113]\n",
      " [0.21280661]\n",
      " [0.41016335]\n",
      " [0.7029967 ]\n",
      " [0.4079629 ]\n",
      " [0.16913842]\n",
      " [0.3175609 ]\n",
      " [0.9742351 ]\n",
      " [0.66758346]\n",
      " [0.56839517]\n",
      " [0.88293115]\n",
      " [0.6835517 ]\n",
      " [0.12649742]\n",
      " [0.05275026]\n",
      " [0.68745236]\n",
      " [0.2897502 ]\n",
      " [0.50186944]\n",
      " [0.28647655]\n",
      " [0.32653446]\n",
      " [0.47345432]\n",
      " [0.34198692]\n",
      " [0.3778638 ]\n",
      " [0.98139543]\n",
      " [0.58261411]\n",
      " [0.40362914]\n",
      " [0.55242502]\n",
      " [0.15320228]\n",
      " [0.25196233]\n",
      " [0.82379774]\n",
      " [0.22298234]] [[0.0639319  0.19937891 0.46760352 0.49013239 0.9207466  0.85969357\n",
      "  0.9739387  0.7263694  0.15555574 0.00765211 0.89733233 0.00273686\n",
      "  0.48512737 0.79013569 0.13391549 0.56796747 0.12385814 0.09661936\n",
      "  0.84045942 0.83852677 0.82058973 0.88455335 0.63001083 0.49286469\n",
      "  0.59441971 0.62546252 0.91279417 0.20055613 0.10408784 0.203241\n",
      "  0.45435889 1.01351033 0.54410143 0.84815746 0.67245995 0.9730982\n",
      "  0.15661936 0.61004627 0.85134169 0.58520709 0.77281764 0.65972231\n",
      "  0.44169985 0.80878768 0.98279028 0.06905271 0.77438436 0.45955407\n",
      "  0.45890168 0.25619989]]\n"
     ]
    }
   ],
   "source": [
    "# Output the reconstructed values of U and V\n",
    "\n",
    "print(U_factorized,V_factorized.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE_missing: 2.1188915087976775e-15\n",
      "RMSE_U_missing: 0.016151981573954834\n",
      "RMSE_V_missing: 0.016973077671913125\n"
     ]
    }
   ],
   "source": [
    "# Compute the RMSE between the original matrix R and the reconstructed matrix R_reconstructed\n",
    "\n",
    "RMSE_missing = np.sqrt(np.mean((R_f - R_reconstructed_missing)**2))\n",
    "\n",
    "print(\"RMSE_missing:\", RMSE_missing)\n",
    "\n",
    "# Compute the RMSE of U and V\n",
    "\n",
    "RMSE_U_missing = np.sqrt(np.mean((u - U_factorized)**2))\n",
    "\n",
    "RMSE_V_missing = np.sqrt(np.mean((v - V_factorized)**2))\n",
    "\n",
    "print(\"RMSE_U_missing:\", RMSE_U_missing)\n",
    "print(\"RMSE_V_missing:\", RMSE_V_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Missing Proportion: 0.1 ********\n",
      "RMSE_missing: 2.4052637664011314e-15\n",
      "RMSE_U_missing: 0.002903838579411324\n",
      "RMSE_V_missing: 0.002980951874122636\n",
      "******* Missing Proportion: 0.2 ********\n",
      "RMSE_missing: 2.9979264203027447e-15\n",
      "RMSE_U_missing: 0.02272930551813848\n",
      "RMSE_V_missing: 0.02416856307586616\n",
      "******* Missing Proportion: 0.3 ********\n",
      "RMSE_missing: 2.0339542859740246e-15\n",
      "RMSE_U_missing: 0.019673670629673425\n",
      "RMSE_V_missing: 0.020804598065549313\n",
      "******* Missing Proportion: 0.4 ********\n",
      "RMSE_missing: 2.5091924606756215e-15\n",
      "RMSE_U_missing: 0.032227756013432245\n",
      "RMSE_V_missing: 0.034866752088528304\n",
      "******* Missing Proportion: 0.5 ********\n",
      "RMSE_missing: 2.0882499147952527e-15\n",
      "RMSE_U_missing: 0.008849151535654218\n",
      "RMSE_V_missing: 0.009179326085041645\n",
      "******* Missing Proportion: 0.6 ********\n",
      "RMSE_missing: 5.634315082766128e-13\n",
      "RMSE_U_missing: 0.004462408611580909\n",
      "RMSE_V_missing: 0.004593396604888851\n",
      "******* Missing Proportion: 0.7 ********\n",
      "RMSE_missing: 3.241459235943753e-08\n",
      "RMSE_U_missing: 0.011058336585723885\n",
      "RMSE_V_missing: 0.01108214058241128\n",
      "******* Missing Proportion: 0.8 ********\n",
      "RMSE_missing: 0.004721950448308407\n",
      "RMSE_U_missing: 0.019795785276497526\n",
      "RMSE_V_missing: 0.0187035809231224\n",
      "******* Missing Proportion: 0.9 ********\n",
      "RMSE_missing: 0.0329380866632767\n",
      "RMSE_U_missing: 0.05879055946297784\n",
      "RMSE_V_missing: 0.017134909186473188\n"
     ]
    }
   ],
   "source": [
    "# Discuss other missing proportions for data generation and discuss their impacts in the reconstruction process\n",
    "\n",
    "# missing protions from 0.1 to 0.9\n",
    "\n",
    "missing_proportions = [0.1*i for i in range(1,10)]\n",
    "\n",
    "for missing_proportion in missing_proportions:\n",
    "\n",
    "    num_missing = int(missing_proportion * total_elements)\n",
    "\n",
    "    missing_indices = np.unravel_index(\n",
    "        np.random.choice(total_elements, num_missing, replace=False), (N, M)\n",
    "    )\n",
    "\n",
    "    R_missing = np.copy(R_f)\n",
    "    \n",
    "    R_missing[missing_indices] = np.nan\n",
    "\n",
    "    U_factorized,V_factorized = SGD_factorization(0.01, 0,1000, R_missing,False,1)\n",
    "\n",
    "    R_reconstructed_missing = U_factorized @ V_factorized.T\n",
    "\n",
    "    RMSE_missing = np.sqrt(np.mean((R_f - R_reconstructed_missing)**2))\n",
    "\n",
    "    print(\"******* Missing Proportion:\", round(missing_proportion,1),\"********\")\n",
    "    print(\"RMSE_missing:\", RMSE_missing)\n",
    "\n",
    "    RMSE_U_missing = np.sqrt(np.mean((u - U_factorized)**2))\n",
    "\n",
    "    RMSE_V_missing = np.sqrt(np.mean((v - V_factorized)**2))\n",
    "\n",
    "    print(\"RMSE_U_missing:\", RMSE_U_missing)\n",
    "\n",
    "    print(\"RMSE_V_missing:\", RMSE_V_missing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: \n",
    "\n",
    "**Reconstructed Matrix**:  \n",
    "1. For the low and moderate missing value proportion such as 10% - 50%, the RMSE is very low, indicating that the factorization can approximate the matrix accuractely. \n",
    "2. For the moderate and high missing proportion such as 60% - 80%, the RMSE increases as the missing proportion increases, showing that a slight degradation in the matrix reconstruction quality. And the degradation becomes notable as the missing proportion reaches 80%.It shows that matrix factorization methods are resilient to a moderate amount of missing data.\n",
    "3. For the very high missing proportion such as 90%, the RMSE rises significantly. The factorization struggles to approximate the original matrix since the data in the matrix becomes pretty sparse, and there is insufficient data to capture the underlying structure effectively.\n",
    "\n",
    "**Reconstructed U and V**:\n",
    "1. For the low missing proportions such as 10% - 30%, the RMSEs for U and V are very low at these missing proportions, indicating that the latent factors U and V are accurately estimated.  \n",
    "2. For the moderate missing proportions such as 40% - 60%, the RMSEs remains very low too at these missing proportions, indicating that the latent factors U and V can be accurately estimated.    \n",
    "3. For the high missing proportions such as 70% - 90%, the RMSEs for U and V becomes relatively high. It shows that for the sparse matrix, the latent factors are difficult to be estimated because the information becomes less and less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Singular Number: 1 ********\n",
      "RMSE_reconstruction: 0.1785662344904074\n",
      "******* Singular Number: 2 ********\n",
      "RMSE_reconstruction: 0.16612216880883932\n",
      "******* Singular Number: 3 ********\n",
      "RMSE_reconstruction: 0.1537641110171347\n",
      "******* Singular Number: 4 ********\n",
      "RMSE_reconstruction: 0.1405963076679247\n",
      "******* Singular Number: 5 ********\n",
      "RMSE_reconstruction: 0.12698460343689363\n",
      "******* Singular Number: 6 ********\n",
      "RMSE_reconstruction: 0.112475592395448\n",
      "******* Singular Number: 7 ********\n",
      "RMSE_reconstruction: 0.09641838014146753\n",
      "******* Singular Number: 8 ********\n",
      "RMSE_reconstruction: 0.07733966356176836\n",
      "******* Singular Number: 9 ********\n",
      "RMSE_reconstruction: 0.05442717199982621\n",
      "******* Singular Number: 10 ********\n",
      "RMSE_reconstruction: 2.460319285765109e-15\n",
      "******* Singular Number: 20 ********\n",
      "RMSE_reconstruction: 2.4639862668001584e-15\n",
      "******* Singular Number: 30 ********\n",
      "RMSE_reconstruction: 2.4676066743274875e-15\n",
      "******* Singular Number: 40 ********\n",
      "RMSE_reconstruction: 2.4701214049155486e-15\n",
      "******* Singular Number: 50 ********\n",
      "RMSE_reconstruction: 2.473262744563341e-15\n"
     ]
    }
   ],
   "source": [
    "# Generate Matrix with Rank 10\n",
    "\n",
    "N,M = 100,50\n",
    "\n",
    "full_rank_matrix = np.random.rand(N, M)\n",
    "    \n",
    "# Perform SVD decomposition\n",
    "U_10, s_10, Vt_10 = np.linalg.svd(full_rank_matrix, full_matrices=False)\n",
    "\n",
    "# Keep only the top `rank` singular values\n",
    "s_10[10:] = 0\n",
    "    \n",
    "# Reconstruct the matrix\n",
    "\n",
    "R_10 = U_10 @ np.diag(s_10) @ Vt_10\n",
    "\n",
    "# choose a moderate missing data proportions\n",
    "num_missing = int(0.3 * total_elements)\n",
    "\n",
    "missing_indices = np.unravel_index(\n",
    "    np.random.choice(total_elements, num_missing, replace=False), (N, M)\n",
    ")\n",
    "\n",
    "R_missing_10 = np.copy(R_10)\n",
    "\n",
    "R_missing_10[missing_indices] = np.nan\n",
    "\n",
    "# decompose the matrix using different number of singular components\n",
    "\n",
    "singular_components = [1,2,3,4,5,6,7,8,9,10, 20, 30, 40, 50]\n",
    "\n",
    "for K in singular_components:\n",
    "\n",
    "    U_reconstructed, s_constructed, V_reconstructed = np.linalg.svd(R_10, full_matrices=False)\n",
    "    \n",
    "    s_constructed[K:] = 0\n",
    "\n",
    "    R_reconstructed_k = U_reconstructed @ np.diag(s_constructed) @ V_reconstructed\n",
    "\n",
    "    RMSE_missing = np.sqrt(np.mean((R_10 - R_reconstructed_k)**2))\n",
    "\n",
    "    print(\"******* Singular Number:\", K ,\"********\")\n",
    "    print(\"RMSE_reconstruction:\", RMSE_missing)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**:   \n",
    "I use the a rank-10 matrix with 30% missing values.\n",
    "1. For the number of singular value smaller than or equal to the Rank of the matrix. As the number of singular value increases, the reconstruction effect increases because we use more features. And the MSE becomes very small as the number of singualr value is equal to the Rank of the matrix.\n",
    "2. For the number of singular values bigger than the Rank of the matrix. As the number of singular value increases, the reconstruction effect remains stable because singular values after the 10th are quite small so there is very little information in those singular components of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
